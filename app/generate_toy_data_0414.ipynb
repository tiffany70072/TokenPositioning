{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4/13: Generate fake numpy files for testing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "30\n",
      "20\n",
      "15\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "data_path = \"../data/\"\n",
    "data_name = \"auto-last-toy\"\n",
    "try:\n",
    "    os.mkdir(os.path.join(data_path, data_name))  # Make directory.\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "\n",
    "max_length = 13\n",
    "vocab_size = 2\n",
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "DUMMY = EOS + vocab_size + 1\n",
    "\n",
    "\n",
    "def get_random(length, candidates=[3, 4]):\n",
    "    def dfs(length, result):\n",
    "        if length == 0:\n",
    "            results.append(result)\n",
    "            return\n",
    "        for candidate in candidates:\n",
    "            dfs(length - 1, result + [candidate])\n",
    "    results = []\n",
    "    dfs(length, [])\n",
    "    return results\n",
    "\n",
    "\n",
    "def turn_to_same_number(lst, target_size=1000): \n",
    "    lst = lst.copy()\n",
    "    if len(lst) < target_size:\n",
    "        m = math.ceil(target_size / len(lst))\n",
    "        n = math.ceil(math.log(m, 2))\n",
    "        #print(m, n)\n",
    "        for i in range(n):\n",
    "            lst = lst + lst.copy()\n",
    "    random.shuffle(lst)\n",
    "    return lst[:target_size]\n",
    "\n",
    "\n",
    "candidates = get_random(3)  \n",
    "print(len(candidates))\n",
    "a = turn_to_same_number(candidates, 30)\n",
    "print(len(a))\n",
    "a = turn_to_same_number(candidates, 20)\n",
    "print(len(a))\n",
    "a = turn_to_same_number(candidates, 15)\n",
    "print(len(a))\n",
    "a = turn_to_same_number(candidates, 5)        \n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000\n",
      "26000\n"
     ]
    }
   ],
   "source": [
    "# Autoenc-last\n",
    "# encoder = [0, 1, x, x, 3, 2]\n",
    "# decoder = [1, 5, 5, 3, 2, 0]\n",
    "data_name = \"auto-last-toy\"\n",
    "min_length = 1\n",
    "max_length = 13\n",
    "target_size = 1000\n",
    "\n",
    "encoder_train = []\n",
    "decoder_train = []\n",
    "for length in range(min_length, max_length + 1):\n",
    "    candidates = get_random(length - 1)\n",
    "    candidates = turn_to_same_number(candidates, target_size)\n",
    "    \n",
    "    for token in range(EOS + 1, EOS + 1 + vocab_size):\n",
    "        encoder_train += [[PAD] * (max_length - length) + [SOS] + candidate + [token, EOS]\n",
    "                          for candidate in candidates]\n",
    "        decoder_train += [[SOS] + [DUMMY] * (length - 1) + [token, EOS] + [PAD] * (max_length - length)\n",
    "                         for _ in range(len(candidates))]\n",
    "\n",
    "print(len(encoder_train))\n",
    "#print(encoder_train)\n",
    "print(len(decoder_train))\n",
    "#print(decoder_train)\n",
    "encoder_valid = np.copy(encoder_train)\n",
    "decoder_valid = np.copy(decoder_train)\n",
    "np.save(\"%s%s/encoder_train.npy\" % (data_path, data_name), encoder_train)\n",
    "np.save(\"%s%s/decoder_train.npy\" % (data_path, data_name), decoder_train)\n",
    "np.save(\"%s%s/encoder_valid.npy\" % (data_path, data_name), encoder_valid)\n",
    "np.save(\"%s%s/decoder_valid.npy\" % (data_path, data_name), decoder_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000\n",
      "26000\n"
     ]
    }
   ],
   "source": [
    "# Token-positioning\n",
    "# encoder = [3, 3]\n",
    "# decoder = [1, x, x, 3, 2, 0, 0]\n",
    "data_name = 'token-posi-toy'\n",
    "try:\n",
    "    os.mkdir(os.path.join(data_path, data_name))  # Make directory.\n",
    "except FileExistsError:\n",
    "    pass\n",
    "min_length = 1\n",
    "max_length = 13\n",
    "target_size = 1000\n",
    "\n",
    "encoder_train = []\n",
    "decoder_train = []\n",
    "for length in range(min_length, max_length + 1):\n",
    "    candidates = get_random(length - 1)\n",
    "    candidates = turn_to_same_number(candidates, target_size)\n",
    "    \n",
    "    for token in range(EOS + 1, EOS + 1 + vocab_size):\n",
    "        encoder_train += [[token, length] for _ in range(len(candidates))]\n",
    "        decoder_train += [[SOS] + candidate + [token, EOS] + [PAD] * (max_length - length)\n",
    "                         for candidate in candidates]\n",
    "\n",
    "print(len(encoder_train))\n",
    "print(len(decoder_train))\n",
    "#print(encoder_train)\n",
    "#print(decoder_train)\n",
    "encoder_valid = np.copy(encoder_train)\n",
    "decoder_valid = np.copy(decoder_train)\n",
    "np.save(\"%s%s/encoder_train.npy\" % (data_path, data_name), encoder_train)\n",
    "np.save(\"%s%s/decoder_train.npy\" % (data_path, data_name), decoder_train)\n",
    "np.save(\"%s%s/encoder_valid.npy\" % (data_path, data_name), encoder_valid)\n",
    "np.save(\"%s%s/decoder_valid.npy\" % (data_path, data_name), decoder_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake data.\n",
    "\n",
    "data_path = \"../data/\"\n",
    "data_name = \"fake-data\"\n",
    "\n",
    "#os.mkdir(os.path.join(data_path, data_name))  # Make directory.\n",
    "\n",
    "encoder_train = np.array([[1, 3, 3, 3, 3, 2],\n",
    "                          [0, 1, 3, 3, 3, 2], \n",
    "                         [0, 0, 1, 3, 3, 2],\n",
    "                         [0, 0, 0, 1, 3, 2],\n",
    "                         [1, 4, 4, 4, 4, 2],\n",
    "                         [0, 1, 4, 4, 4, 2],\n",
    "                         [0, 0, 1, 4, 4, 2],\n",
    "                         [0, 0, 0, 1, 4, 2]])\n",
    "decoder_train = np.array([[1, 5, 5, 5, 3, 2],\n",
    "                          [1, 5, 5, 3, 2, 0], \n",
    "                         [1, 5, 3, 2, 0, 0],\n",
    "                         [1, 3, 2, 0, 0, 0],\n",
    "                         [1, 5, 5, 5, 4, 2],\n",
    "                         [1, 5, 5, 4, 2, 0],\n",
    "                         [1, 5, 4, 2, 0, 0],\n",
    "                         [1, 4, 2, 0, 0, 0]])\n",
    "encoder_valid = np.copy(encoder_train)\n",
    "decoder_valid = np.copy(decoder_train)\n",
    "np.save(\"%s%s/encoder_train.npy\" % (data_path, data_name), encoder_train)\n",
    "np.save(\"%s%s/decoder_train.npy\" % (data_path, data_name), decoder_train)\n",
    "np.save(\"%s%s/encoder_valid.npy\" % (data_path, data_name), encoder_valid)\n",
    "np.save(\"%s%s/decoder_valid.npy\" % (data_path, data_name), decoder_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type = encoder_train, data_shape = [100, 8]\n",
      "data_type = decoder_train, data_shape = [100, 8]\n",
      "data_type = encoder_valid, data_shape = [50, 8]\n",
      "data_type = decoder_valid, data_shape = [50, 8]\n"
     ]
    }
   ],
   "source": [
    "# Generate fake data.\n",
    "\n",
    "data_path = \"../data/\"\n",
    "data_name = \"fake-data-2\"\n",
    "data_type_list = ['encoder_train', 'decoder_train', 'encoder_valid', 'decoder_valid']\n",
    "data_shape_list = [[100, 8], [100, 8], [50, 8], [50, 8]]\n",
    "\n",
    "os.mkdir(os.path.join(data_path, data_name))  # Make directory.\n",
    "\n",
    "for data_type, data_shape in zip(data_type_list, data_shape_list):\n",
    "    print(\"data_type = %s, data_shape =\" % data_type, data_shape)\n",
    "    npy_file = np.ones(data_shape)\n",
    "    np.save(\"%s%s/%s.npy\" % (data_path, data_name, data_type), npy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test src/main.py with task = \"autoenc-last\":\n",
    "# In src/, run $ python3 main.py --task=autoenc-last --units=10 --max_epochs=1 --mode=train --data_name=fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 ? -> 1.0\n",
      "0.0 ? -> 0.0\n",
      "1.0 ? -> 1.0\n",
      "0.0 ? -> 0.0\n",
      "0.5 ? -> 0.5\n"
     ]
    }
   ],
   "source": [
    "# Test evaluate_autoencoder_last_step.\n",
    "\n",
    "import evaluator\n",
    "import importlib\n",
    "importlib.reload(evaluator)\n",
    "\n",
    "real = np.array([[3, 3, 3, 2, 0, 0]])  # 0: PAD, 2: EOS, 3, 4: vocab\n",
    "pred = np.array([[3, 3, 3, 2, 0, 0]])\n",
    "acc = evaluator.evaluate_autoencoder_last_step(real, pred)\n",
    "print(\"1.0 ? ->\", acc)\n",
    "\n",
    "real = np.array([[3, 3, 3, 2, 0, 0]])  # The last words are different.\n",
    "pred = np.array([[3, 3, 4, 2, 0, 0]])\n",
    "acc = evaluator.evaluate_autoencoder_last_step(real, pred)\n",
    "print(\"0.0 ? ->\", acc)\n",
    "\n",
    "\n",
    "real = np.array([[3, 3, 3, 2, 0, 0]])  # Other words are different.\n",
    "pred = np.array([[4, 4, 3, 2, 0, 0]])\n",
    "acc = evaluator.evaluate_autoencoder_last_step(real, pred)\n",
    "print(\"1.0 ? ->\", acc)\n",
    "\n",
    "\n",
    "real = np.array([[3, 3, 3, 2, 0, 0]])  # Every word are same, but last word appear at wrong t.\n",
    "pred = np.array([[3, 3, 3, 3, 2, 2]])\n",
    "acc = evaluator.evaluate_autoencoder_last_step(real, pred)\n",
    "print(\"0.0 ? ->\", acc)\n",
    "\n",
    "\n",
    "real = np.array([[3, 3, 3, 2, 0, 0], [3, 3, 3, 2, 0, 0]])  \n",
    "pred = np.array([[3, 3, 3, 3, 2, 2], [4, 4, 3, 2, 0, 0]])\n",
    "acc = evaluator.evaluate_autoencoder_last_step(real, pred)\n",
    "print(\"0.5 ? ->\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
